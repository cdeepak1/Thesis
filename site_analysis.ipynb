{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c690d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Site Country  Total Years  Max Continuous Years\n",
      "0   AT-Neu      AT           11                    11\n",
      "1   BE-Bra      BE           19                    19\n",
      "2   BE-Lon      BE           11                    11\n",
      "3   BE-Vie      BE           19                    19\n",
      "4   CH-Cha      CH           10                    10\n",
      "..     ...     ...          ...                   ...\n",
      "57  IT-SRo      IT           14                    14\n",
      "58  IT-Tor      IT            7                     7\n",
      "59  NL-Hor      NL            8                     8\n",
      "60  NL-Loo      NL           19                    19\n",
      "61  SE-St1      SE            3                     3\n",
      "\n",
      "[62 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('data-availability-20250710115327.csv', index_col='Year/Site ID')\n",
    "\n",
    "# Define European countries (based on site prefixes)\n",
    "european_prefixes = ['AT', 'BE', 'CH', 'CZ', 'DE', 'DK', 'FI', 'FR', 'IT', 'NL', 'SE']\n",
    "\n",
    "# Filter European sites - CORRECTED version\n",
    "europe_sites = [site for site in df.index if any(site.startswith(prefix) for prefix in european_prefixes)]\n",
    "europe_df = df.loc[europe_sites]\n",
    "\n",
    "# Function to calculate continuous years\n",
    "def max_continuous_years(row):\n",
    "    years = [int(col) for col in df.columns if str(row[col]) in ['+', 'Tier 2']]\n",
    "    if not years:\n",
    "        return 0\n",
    "    years_sorted = sorted(years)\n",
    "    max_streak = current_streak = 1\n",
    "    for i in range(1, len(years_sorted)):\n",
    "        if years_sorted[i] == years_sorted[i-1] + 1:\n",
    "            current_streak += 1\n",
    "            max_streak = max(max_streak, current_streak)\n",
    "        else:\n",
    "            current_streak = 1\n",
    "    return max_streak\n",
    "\n",
    "# Apply calculations\n",
    "results = []\n",
    "for site in europe_df.index:\n",
    "    total_years = europe_df.loc[site].isin(['+', 'Tier 2']).sum()\n",
    "    continuous_years = max_continuous_years(europe_df.loc[site])\n",
    "    results.append({\n",
    "        'Site': site,\n",
    "        'Country': site.split('-')[0],\n",
    "        'Total Years': total_years,\n",
    "        'Max Continuous Years': continuous_years\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame and save\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('european_sites_analysis.csv', index=False)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "269c2649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Site Country  Total Years  Max Continuous Years  First Year  Last Year  \\\n",
      "1   BE-Bra      BE           19                    19        1996       2014   \n",
      "3   BE-Vie      BE           19                    19        1996       2014   \n",
      "26  DE-Tha      DE           19                    19        1996       2014   \n",
      "44  IT-Col      IT           19                    19        1996       2014   \n",
      "31  FI-Hyy      FI           19                    19        1996       2014   \n",
      "..     ...     ...          ...                   ...         ...        ...   \n",
      "48  IT-La2      IT            3                     3        2000       2002   \n",
      "47  IT-Isp      IT            2                     2        2013       2014   \n",
      "27  DE-Zrk      DE            2                     2        2013       2014   \n",
      "56  IT-SR2      IT            2                     2        2013       2014   \n",
      "29  DK-Fou      DK            1                     1        2005       2005   \n",
      "\n",
      "   Year Range  \n",
      "1   1996-2014  \n",
      "3   1996-2014  \n",
      "26  1996-2014  \n",
      "44  1996-2014  \n",
      "31  1996-2014  \n",
      "..        ...  \n",
      "48  2000-2002  \n",
      "47  2013-2014  \n",
      "27  2013-2014  \n",
      "56  2013-2014  \n",
      "29  2005-2005  \n",
      "\n",
      "[62 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('data-availability-20250710115327.csv', index_col='Year/Site ID')\n",
    "\n",
    "# Define European countries (based on site prefixes)\n",
    "european_prefixes = ['AT', 'BE', 'CH', 'CZ', 'DE', 'DK', 'FI', 'FR', 'IT', 'NL', 'SE']\n",
    "\n",
    "# Filter European sites\n",
    "europe_sites = [site for site in df.index if any(site.startswith(prefix) for prefix in european_prefixes)]\n",
    "europe_df = df.loc[europe_sites]\n",
    "\n",
    "# Function to calculate year statistics\n",
    "def get_year_stats(row):\n",
    "    years = [int(col) for col in df.columns if str(row[col]) in ['+', 'Tier 2']]\n",
    "    if not years:\n",
    "        return 0, 0, 0, \"No data\"\n",
    "    \n",
    "    years_sorted = sorted(years)\n",
    "    first_year = years_sorted[0]\n",
    "    last_year = years_sorted[-1]\n",
    "    year_range = f\"{first_year}-{last_year}\"\n",
    "    \n",
    "    # Calculate continuous years\n",
    "    max_streak = current_streak = 1\n",
    "    for i in range(1, len(years_sorted)):\n",
    "        if years_sorted[i] == years_sorted[i-1] + 1:\n",
    "            current_streak += 1\n",
    "            max_streak = max(max_streak, current_streak)\n",
    "        else:\n",
    "            current_streak = 1\n",
    "            \n",
    "    return len(years), max_streak, first_year, last_year, year_range\n",
    "\n",
    "# Apply calculations\n",
    "results = []\n",
    "for site in europe_df.index:\n",
    "    total_years, continuous_years, first_year, last_year, year_range = get_year_stats(europe_df.loc[site])\n",
    "    results.append({\n",
    "        'Site': site,\n",
    "        'Country': site.split('-')[0],\n",
    "        'Total Years': total_years,\n",
    "        'Max Continuous Years': continuous_years,\n",
    "        'First Year': first_year,\n",
    "        'Last Year': last_year,\n",
    "        'Year Range': year_range\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame and save\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values(by='Total Years', ascending=False)  # Sort by total years\n",
    "results_df.to_csv('european_sites_analysis_with_years.csv', index=False)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f30e9d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing files in: C:\\Deepak\\icos\\icos\n",
      "\n",
      "Successfully processed 31 files\n",
      "\n",
      "=== Analysis Complete ===\n",
      "Site-level statistics saved to: C:\\Deepak\\icos\\icos\\analysis_results\\site_statistics.csv\n",
      "Country-level statistics saved to: C:\\Deepak\\icos\\icos\\analysis_results\\country_statistics.csv\n",
      "\n",
      "Sample of Site Statistics:\n",
      "  Site Code Country  Duration  Max Continuous Years Year Range\n",
      "0    BE-Bra      BE        25                    25  1996-2020\n",
      "1    BE-Dor      BE        10                    10  2011-2020\n",
      "2    BE-Lon      BE        17                    17  2004-2020\n",
      "3    BE-Maa      BE         5                     5  2016-2020\n",
      "4    BE-Vie      BE        25                    25  1996-2020\n",
      "\n",
      "Country Summary Statistics:\n",
      "  Country  Number of Sites                                           Sites  \\\n",
      "0      BE                5          BE-Bra, BE-Dor, BE-Lon, BE-Maa, BE-Vie   \n",
      "1      CH                1                                          CH-Dav   \n",
      "2      CZ                2                                  CZ-BK1, CZ-Lnz   \n",
      "3      DE                4                  DE-Geb, DE-HoH, DE-RuS, DE-Tha   \n",
      "4      DK                1                                          DK-Sor   \n",
      "5      FI                2                                  FI-Hyy, FI-Sii   \n",
      "6      FR                6  FR-Bil, FR-FBn, FR-Fon, FR-Gri, FR-Hes, FR-Lam   \n",
      "7      IT                5          IT-BCi, IT-Cp2, IT-MBo, IT-Ren, IT-SR2   \n",
      "8      NL                1                                          NL-Loo   \n",
      "9      SE                4                  SE-Deg, SE-Htm, SE-Nor, SE-Svb   \n",
      "\n",
      "   First Year  Last Year  Total Years   Coverage  \n",
      "0        1996       2020           25  1996-2020  \n",
      "1        1997       2020           24  1997-2020  \n",
      "2        2004       2020           17  2004-2020  \n",
      "3        1996       2020           25  1996-2020  \n",
      "4        1996       2020           25  1996-2020  \n",
      "5        1996       2020           25  1996-2020  \n",
      "6        2004       2020           17  2004-2020  \n",
      "7        1999       2020           22  1999-2020  \n",
      "8        1996       2018           23  1996-2018  \n",
      "9        2001       2020           20  2001-2020  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "def analyze_flux_files(directory):\n",
    "    results = []\n",
    "    country_stats = defaultdict(lambda: {'sites': set(), 'years': set()})\n",
    "    processed_files = 0\n",
    "    \n",
    "    for filename in os.listdir(directory):\n",
    "        if not (filename.startswith('FLX_') and filename.endswith('.csv')):\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # Split filename into components\n",
    "            parts = filename.split('_')\n",
    "            if len(parts) < 5:\n",
    "                print(f\"Skipping {filename}: not enough components\")\n",
    "                continue\n",
    "                \n",
    "            # Extract site information\n",
    "            site_code = parts[1]\n",
    "            country = site_code.split('-')[0]\n",
    "            \n",
    "            # Find the part that contains the year range\n",
    "            year_part = None\n",
    "            for part in parts:\n",
    "                if '-' in part and part.replace('-', '').isdigit():\n",
    "                    year_part = part\n",
    "                    break\n",
    "            \n",
    "            if not year_part:\n",
    "                print(f\"Skipping {filename}: no year range found\")\n",
    "                continue\n",
    "                \n",
    "            # Extract years\n",
    "            start_year, end_year = map(int, year_part.split('-'))\n",
    "            duration = end_year - start_year + 1\n",
    "            \n",
    "            # Update statistics\n",
    "            country_stats[country]['sites'].add(site_code)\n",
    "            country_stats[country]['years'].update(range(start_year, end_year + 1))\n",
    "            \n",
    "            results.append({\n",
    "                'Filename': filename,\n",
    "                'Site Code': site_code,\n",
    "                'Country': country,\n",
    "                'Start Year': start_year,\n",
    "                'End Year': end_year,\n",
    "                'Duration': duration,\n",
    "                'Year Range': f\"{start_year}-{end_year}\"\n",
    "            })\n",
    "            processed_files += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {str(e)}\")\n",
    "    \n",
    "    print(f\"\\nSuccessfully processed {processed_files} files\")\n",
    "    return results, country_stats\n",
    "\n",
    "def generate_outputs(results, country_stats, directory):\n",
    "    if not results:\n",
    "        print(\"\\nNo valid flux tower files were processed.\")\n",
    "        return\n",
    "    \n",
    "    # Create detailed dataframe\n",
    "    df = pd.DataFrame(results)\n",
    "    df['Max Continuous Years'] = df['Duration']  # Filenames contain continuous ranges\n",
    "    detailed_cols = ['Site Code', 'Country', 'Duration', 'Max Continuous Years', 'Year Range']\n",
    "    df = df[detailed_cols].sort_values(['Country', 'Site Code'])\n",
    "    \n",
    "    # Create summary dataframe\n",
    "    summary_data = []\n",
    "    for country, stats in country_stats.items():\n",
    "        years = sorted(stats['years'])\n",
    "        summary_data.append({\n",
    "            'Country': country,\n",
    "            'Number of Sites': len(stats['sites']),\n",
    "            'Sites': ', '.join(sorted(stats['sites'])),\n",
    "            'First Year': min(years),\n",
    "            'Last Year': max(years),\n",
    "            'Total Years': len(years),\n",
    "            'Coverage': f\"{min(years)}-{max(years)}\"\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data).sort_values('Country')\n",
    "    \n",
    "    # Save outputs\n",
    "    output_dir = os.path.join(directory, 'analysis_results')\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    site_output = os.path.join(output_dir, 'site_statistics.csv')\n",
    "    country_output = os.path.join(output_dir, 'country_statistics.csv')\n",
    "    \n",
    "    df.to_csv(site_output, index=False)\n",
    "    summary_df.to_csv(country_output, index=False)\n",
    "    \n",
    "    print(\"\\n=== Analysis Complete ===\")\n",
    "    print(f\"Site-level statistics saved to: {site_output}\")\n",
    "    print(f\"Country-level statistics saved to: {country_output}\")\n",
    "    \n",
    "    print(\"\\nSample of Site Statistics:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    print(\"\\nCountry Summary Statistics:\")\n",
    "    print(summary_df)\n",
    "\n",
    "# Run the analysis\n",
    "directory = r'C:\\Deepak\\icos\\icos'\n",
    "print(f\"Analyzing files in: {directory}\")\n",
    "results, country_stats = analyze_flux_files(directory)\n",
    "generate_outputs(results, country_stats, directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5cbf4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing ICOS Carbon Portal data from: Carbon_Portal_Search_Result.csv\n",
      "\n",
      "Successfully processed 421 entries\n",
      "\n",
      "=== Analysis Complete ===\n",
      "Detailed statistics saved to: icos_analysis_results\\detailed_statistics.csv\n",
      "Site summary saved to: icos_analysis_results\\site_summary.csv\n",
      "Country summary saved to: icos_analysis_results\\country_summary.csv\n",
      "\n",
      "Sample of Detailed Statistics:\n",
      "    Site Code Country       Product Type  Duration Year Range  Size (bytes)  \\\n",
      "15     BE-Bra      BE  etcArchiveProduct         6  2020-2025      98188780   \n",
      "420    BE-Bra      BE       etcL2AuxData         6  2020-2025       7040375   \n",
      "419    BE-Bra      BE        etcL2Fluxes         6  2019-2024      16907118   \n",
      "416    BE-Bra      BE       etcL2Fluxnet         6  2019-2024      41948459   \n",
      "418    BE-Bra      BE         etcL2Meteo         6  2019-2024       7141933   \n",
      "\n",
      "              Submission Date  \n",
      "15   2025-04-28T14:27:00.546Z  \n",
      "420  2025-04-01T08:45:16.036Z  \n",
      "419  2025-04-01T08:49:13.247Z  \n",
      "416  2025-04-01T08:56:25.055Z  \n",
      "418  2025-04-01T08:50:06.283Z  \n",
      "\n",
      "Site Summary Statistics:\n",
      "   Site Code Country  First Year  Last Year  Total Years   Coverage  \\\n",
      "15    BE-Bra      BE        2019       2025            7  2019-2025   \n",
      "78    BE-Dor      BE        2021       2025            5  2021-2025   \n",
      "14    BE-Lon      BE        2016       2025           10  2016-2025   \n",
      "77    BE-Maa      BE        2019       2025            7  2019-2025   \n",
      "13    BE-Vie      BE        2019       2025            7  2019-2025   \n",
      "\n",
      "    Total Size (MB)  \n",
      "15           183.89  \n",
      "78            79.99  \n",
      "14           212.72  \n",
      "77           165.29  \n",
      "13           172.98  \n",
      "\n",
      "Country Summary Statistics:\n",
      "   Country  Number of Sites  \\\n",
      "8       BE                5   \n",
      "11      CD                1   \n",
      "7       CH                2   \n",
      "6       CZ                3   \n",
      "5       DE               15   \n",
      "4       DK                2   \n",
      "14      ES                1   \n",
      "3       FI                9   \n",
      "2       FR               13   \n",
      "9       GF                1   \n",
      "10      GL                4   \n",
      "12      GR                2   \n",
      "1       IT               13   \n",
      "16      NL                1   \n",
      "13      NO                1   \n",
      "0       SE                5   \n",
      "15      UK                1   \n",
      "\n",
      "                                                Sites  First Year  Last Year  \\\n",
      "8              BE-Bra, BE-Dor, BE-Lon, BE-Maa, BE-Vie        2016       2025   \n",
      "11                                             CD-Ygb        2019       2025   \n",
      "7                                      CH-BaK, CH-Dav        2015       2025   \n",
      "6                              CZ-BK1, CZ-Lnz, CZ-wet        2019       2025   \n",
      "5   DE-Amv, DE-BeR, DE-Brs, DE-Geb, DE-Gri, DE-GsB...        2010       2025   \n",
      "4                                      DK-RCW, DK-Sor        2020       2025   \n",
      "14                                             ES-LMa        2019       2025   \n",
      "3   FI-Hyy, FI-Ken, FI-Kmp, FI-Kvr, FI-Let, FI-Sii...        2016       2025   \n",
      "2   FR-Aur, FR-Bil, FR-CLt, FR-EM2, FR-FBn, FR-Fon...        2016       2025   \n",
      "9                                              GF-Guy        2016       2025   \n",
      "10                     GL-Dsk, GL-NuF, GL-ZaF, GL-ZaH        2018       2025   \n",
      "12                                     GR-HeK, GR-HeM        2021       2025   \n",
      "1   IT-BCi, IT-BFt, IT-Cp2, IT-Lsn, IT-MBo, IT-Niv...        2004       2025   \n",
      "16                                             NL-Loo        2022       2025   \n",
      "13                                             NO-Hur        2022       2025   \n",
      "0              SE-Deg, SE-Htm, SE-Nor, SE-Sto, SE-Svb        2017       2025   \n",
      "15                                             UK-AMo        2021       2025   \n",
      "\n",
      "    Total Years   Coverage  Total Size (GB)  \n",
      "8            10  2016-2025             0.80  \n",
      "11            7  2019-2025             0.10  \n",
      "7            11  2015-2025             0.27  \n",
      "6             7  2019-2025             0.35  \n",
      "5            16  2010-2025             2.19  \n",
      "4             6  2020-2025             0.15  \n",
      "14            7  2019-2025             0.12  \n",
      "3            10  2016-2025             1.00  \n",
      "2            10  2016-2025             2.00  \n",
      "9            10  2016-2025             0.10  \n",
      "10            8  2018-2025             0.28  \n",
      "12            5  2021-2025             0.03  \n",
      "1            22  2004-2025             1.60  \n",
      "16            4  2022-2025             0.06  \n",
      "13            4  2022-2025             0.04  \n",
      "0             9  2017-2025             0.80  \n",
      "15            5  2021-2025             0.11  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "def analyze_icos_files(csv_path):\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    results = []\n",
    "    site_stats = defaultdict(lambda: {'products': set(), 'years': set(), 'size': 0})\n",
    "    country_stats = defaultdict(lambda: {'sites': set(), 'years': set(), 'size': 0})\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        try:\n",
    "            # Extract site information from filename\n",
    "            filename = row['fileName']\n",
    "            if not filename.startswith('ICOSETC_'):\n",
    "                continue\n",
    "                \n",
    "            parts = filename.split('_')\n",
    "            site_code = parts[1]\n",
    "            country = site_code.split('-')[0]\n",
    "            \n",
    "            # Extract product type from spec URL\n",
    "            product_type = row['spec'].split('/')[-1]\n",
    "            \n",
    "            # Parse dates\n",
    "            start_date = datetime.strptime(row['timeStart'], '%Y-%m-%dT%H:%M:%SZ')\n",
    "            end_date = datetime.strptime(row['timeEnd'], '%Y-%m-%dT%H:%M:%SZ')\n",
    "            start_year = start_date.year\n",
    "            end_year = end_date.year\n",
    "            duration = end_year - start_year + 1\n",
    "            \n",
    "            # Update statistics\n",
    "            site_stats[site_code]['products'].add(product_type)\n",
    "            site_stats[site_code]['years'].update(range(start_year, end_year + 1))\n",
    "            site_stats[site_code]['size'] += row['size']\n",
    "            \n",
    "            country_stats[country]['sites'].add(site_code)\n",
    "            country_stats[country]['years'].update(range(start_year, end_year + 1))\n",
    "            country_stats[country]['size'] += row['size']\n",
    "            \n",
    "            results.append({\n",
    "                'Filename': filename,\n",
    "                'Site Code': site_code,\n",
    "                'Country': country,\n",
    "                'Product Type': product_type,\n",
    "                'Start Year': start_year,\n",
    "                'End Year': end_year,\n",
    "                'Duration': duration,\n",
    "                'Year Range': f\"{start_year}-{end_year}\",\n",
    "                'Size (bytes)': row['size'],\n",
    "                'Submission Date': row['submTime']\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row {_}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nSuccessfully processed {len(results)} entries\")\n",
    "    return results, site_stats, country_stats\n",
    "\n",
    "def generate_outputs(results, site_stats, country_stats, output_dir):\n",
    "    if not results:\n",
    "        print(\"\\nNo valid entries were processed.\")\n",
    "        return\n",
    "    \n",
    "    # Create detailed dataframe\n",
    "    df = pd.DataFrame(results)\n",
    "    detailed_cols = ['Site Code', 'Country', 'Product Type', 'Duration', \n",
    "                    'Year Range', 'Size (bytes)', 'Submission Date']\n",
    "    df = df[detailed_cols].sort_values(['Country', 'Site Code', 'Product Type'])\n",
    "    \n",
    "    # Create site summary dataframe\n",
    "    site_data = []\n",
    "    for site, stats in site_stats.items():\n",
    "        years = sorted(stats['years'])\n",
    "        site_data.append({\n",
    "            'Site Code': site,\n",
    "            'Country': site.split('-')[0],\n",
    "            'First Year': min(years),\n",
    "            'Last Year': max(years),\n",
    "            'Total Years': len(years),\n",
    "            'Coverage': f\"{min(years)}-{max(years)}\",\n",
    "            'Total Size (MB)': round(stats['size'] / (1024*1024), 2)\n",
    "        })\n",
    "    \n",
    "    site_summary_df = pd.DataFrame(site_data).sort_values(['Country', 'Site Code'])\n",
    "    \n",
    "    # Create country summary dataframe\n",
    "    country_data = []\n",
    "    for country, stats in country_stats.items():\n",
    "        years = sorted(stats['years'])\n",
    "        country_data.append({\n",
    "            'Country': country,\n",
    "            'Number of Sites': len(stats['sites']),\n",
    "            'Sites': ', '.join(sorted(stats['sites'])),\n",
    "            'First Year': min(years),\n",
    "            'Last Year': max(years),\n",
    "            'Total Years': len(years),\n",
    "            'Coverage': f\"{min(years)}-{max(years)}\",\n",
    "            'Total Size (GB)': round(stats['size'] / (1024*1024*1024), 2)\n",
    "        })\n",
    "    \n",
    "    country_summary_df = pd.DataFrame(country_data).sort_values('Country')\n",
    "    \n",
    "    # Save outputs\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    detailed_output = os.path.join(output_dir, 'detailed_statistics.csv')\n",
    "    site_output = os.path.join(output_dir, 'site_summary.csv')\n",
    "    country_output = os.path.join(output_dir, 'country_summary.csv')\n",
    "    \n",
    "    df.to_csv(detailed_output, index=False)\n",
    "    site_summary_df.to_csv(site_output, index=False)\n",
    "    country_summary_df.to_csv(country_output, index=False)\n",
    "    \n",
    "    print(\"\\n=== Analysis Complete ===\")\n",
    "    print(f\"Detailed statistics saved to: {detailed_output}\")\n",
    "    print(f\"Site summary saved to: {site_output}\")\n",
    "    print(f\"Country summary saved to: {country_output}\")\n",
    "    \n",
    "    print(\"\\nSample of Detailed Statistics:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    print(\"\\nSite Summary Statistics:\")\n",
    "    print(site_summary_df.head())\n",
    "    \n",
    "    print(\"\\nCountry Summary Statistics:\")\n",
    "    print(country_summary_df)\n",
    "\n",
    "# Run the analysis\n",
    "csv_path = 'Carbon_Portal_Search_Result.csv'  # Update with your actual path\n",
    "output_dir = 'icos_analysis_results'\n",
    "\n",
    "print(f\"Analyzing ICOS Carbon Portal data from: {csv_path}\")\n",
    "results, site_stats, country_stats = analyze_icos_files(csv_path)\n",
    "generate_outputs(results, site_stats, country_stats, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60fadcd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing FLUXNET files in: C:\\Deepak\\Drought-2018 ecosystem eddy covariance flux product for 52 stations in FLUXNET-Archive format—release 2019-2\n",
      "\n",
      "Successfully processed 52 files\n",
      "\n",
      "=== Analysis Complete ===\n",
      "Detailed statistics saved to: C:\\Deepak\\Drought-2018 ecosystem eddy covariance flux product for 52 stations in FLUXNET-Archive format—release 2019-2\\analysis_results\\fluxnet_detailed_stats.csv\n",
      "Site summary saved to: C:\\Deepak\\Drought-2018 ecosystem eddy covariance flux product for 52 stations in FLUXNET-Archive format—release 2019-2\\analysis_results\\fluxnet_site_summary.csv\n",
      "Country summary saved to: C:\\Deepak\\Drought-2018 ecosystem eddy covariance flux product for 52 stations in FLUXNET-Archive format—release 2019-2\\analysis_results\\fluxnet_country_summary.csv\n",
      "\n",
      "Sample of Detailed Statistics:\n",
      "                                            Filename Site Code Country  \\\n",
      "0  FLX_BE-Bra_FLUXNET2015_FULLSET_1996-2018_beta-...    BE-Bra      BE   \n",
      "1  FLX_BE-Lon_FLUXNET2015_FULLSET_2004-2018_beta-...    BE-Lon      BE   \n",
      "2  FLX_BE-Vie_FLUXNET2015_FULLSET_1996-2018_beta-...    BE-Vie      BE   \n",
      "3  FLX_CH-Aws_FLUXNET2015_FULLSET_2010-2018_beta-...    CH-Aws      CH   \n",
      "4  FLX_CH-Cha_FLUXNET2015_FULLSET_2005-2018_beta-...    CH-Cha      CH   \n",
      "\n",
      "               Dataset Version  Duration (years) Year Range  Size (MB)  \n",
      "0  FLUXNET2015_FULLSET       3                23  1996-2018     174.52  \n",
      "1  FLUXNET2015_FULLSET       3                15  2004-2018     145.63  \n",
      "2  FLUXNET2015_FULLSET       3                23  1996-2018     182.90  \n",
      "3  FLUXNET2015_FULLSET       3                 9  2010-2018      55.53  \n",
      "4  FLUXNET2015_FULLSET       3                14  2005-2018     135.76  \n",
      "\n",
      "Site Summary Statistics:\n",
      "  Site Code Country Dataset Version  First Year  Last Year  Total Years  \\\n",
      "0    BE-Bra      BE               3        1996       2018           23   \n",
      "1    BE-Lon      BE               3        2004       2018           15   \n",
      "2    BE-Vie      BE               3        1996       2018           23   \n",
      "3    CH-Aws      CH               3        2010       2018            9   \n",
      "4    CH-Cha      CH               3        2005       2018           14   \n",
      "\n",
      "    Coverage  Total Size (GB)  \n",
      "0  1996-2018             0.17  \n",
      "1  2004-2018             0.14  \n",
      "2  1996-2018             0.18  \n",
      "3  2010-2018             0.05  \n",
      "4  2005-2018             0.13  \n",
      "\n",
      "Country Summary Statistics:\n",
      "   Country  Number of Sites  \\\n",
      "0       BE                3   \n",
      "1       CH                6   \n",
      "2       CZ                5   \n",
      "3       DE               13   \n",
      "4       DK                1   \n",
      "5       ES                3   \n",
      "6       FI                4   \n",
      "7       FR                3   \n",
      "8       IT                5   \n",
      "9       NL                1   \n",
      "10      RU                2   \n",
      "11      SE                6   \n",
      "\n",
      "                                                Sites  First Year  Last Year  \\\n",
      "0                              BE-Bra, BE-Lon, BE-Vie        1996       2018   \n",
      "1      CH-Aws, CH-Cha, CH-Dav, CH-Fru, CH-Lae, CH-Oe2        1997       2018   \n",
      "2              CZ-BK1, CZ-Lnz, CZ-RAJ, CZ-Stn, CZ-wet        2004       2018   \n",
      "3   DE-Akm, DE-Geb, DE-Gri, DE-Hai, DE-HoH, DE-Hte...        1996       2018   \n",
      "4                                              DK-Sor        1996       2018   \n",
      "5                              ES-Abr, ES-LM1, ES-LM2        2014       2018   \n",
      "6                      FI-Hyy, FI-Let, FI-Sii, FI-Var        1996       2018   \n",
      "7                              FR-Bil, FR-EM2, FR-Hes        2014       2018   \n",
      "8              IT-BCi, IT-Cp2, IT-Lsn, IT-SR2, IT-Tor        2004       2018   \n",
      "9                                              NL-Loo        1996       2018   \n",
      "10                                     RU-Fy2, RU-Fyo        1998       2018   \n",
      "11     SE-Deg, SE-Htm, SE-Lnn, SE-Nor, SE-Ros, SE-Svb        2001       2018   \n",
      "\n",
      "    Total Years   Coverage  Total Size (GB)  \n",
      "0            23  1996-2018             0.49  \n",
      "1            22  1997-2018             0.79  \n",
      "2            15  2004-2018             0.45  \n",
      "3            23  1996-2018             1.40  \n",
      "4            23  1996-2018             0.19  \n",
      "5             5  2014-2018             0.15  \n",
      "6            23  1996-2018             0.34  \n",
      "7             5  2014-2018             0.13  \n",
      "8            15  2004-2018             0.39  \n",
      "9            23  1996-2018             0.20  \n",
      "10           21  1998-2018             0.23  \n",
      "11           18  2001-2018             0.40  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "def analyze_fluxnet_files(directory):\n",
    "    results = []\n",
    "    site_stats = defaultdict(lambda: {'years': set(), 'size': 0, 'versions': set()})\n",
    "    country_stats = defaultdict(lambda: {'sites': set(), 'years': set(), 'size': 0})\n",
    "    \n",
    "    for filename in os.listdir(directory):\n",
    "        if not (filename.startswith('FLX_') and filename.endswith('.zip')):\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # Split filename into components\n",
    "            parts = filename.split('_')\n",
    "            if len(parts) < 5:\n",
    "                print(f\"Skipping {filename}: not enough components\")\n",
    "                continue\n",
    "                \n",
    "            # Extract site information\n",
    "            site_code = parts[1]\n",
    "            country = site_code.split('-')[0]\n",
    "            \n",
    "            # Extract dataset version\n",
    "            version_part = parts[-1].replace('.zip', '')\n",
    "            version = version_part.split('-')[-1]  # e.g., beta-3\n",
    "            \n",
    "            # Extract year range - it's in the 4th part (index 3) after splitting\n",
    "            year_part = parts[4]  # Changed from parts[3] to parts[4]\n",
    "            start_year, end_year = map(int, year_part.split('-'))\n",
    "            duration = end_year - start_year + 1\n",
    "            \n",
    "            # Get file size\n",
    "            file_size = os.path.getsize(os.path.join(directory, filename))\n",
    "            \n",
    "            # Update statistics\n",
    "            site_stats[site_code]['years'].update(range(start_year, end_year + 1))\n",
    "            site_stats[site_code]['size'] += file_size\n",
    "            site_stats[site_code]['versions'].add(version)\n",
    "            \n",
    "            country_stats[country]['sites'].add(site_code)\n",
    "            country_stats[country]['years'].update(range(start_year, end_year + 1))\n",
    "            country_stats[country]['size'] += file_size\n",
    "            \n",
    "            results.append({\n",
    "                'Filename': filename,\n",
    "                'Site Code': site_code,\n",
    "                'Country': country,\n",
    "                'Dataset': 'FLUXNET2015_FULLSET',\n",
    "                'Version': version,\n",
    "                'Start Year': start_year,\n",
    "                'End Year': end_year,\n",
    "                'Duration (years)': duration,\n",
    "                'Year Range': f\"{start_year}-{end_year}\",\n",
    "                'Size (bytes)': file_size,\n",
    "                'Size (MB)': round(file_size / (1024*1024), 2)\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nSuccessfully processed {len(results)} files\")\n",
    "    return results, site_stats, country_stats\n",
    "\n",
    "def generate_outputs(results, site_stats, country_stats, output_dir):\n",
    "    if not results:\n",
    "        print(\"\\nNo valid FLUXNET files were processed.\")\n",
    "        return\n",
    "    \n",
    "    # Create detailed dataframe\n",
    "    df = pd.DataFrame(results)\n",
    "    detailed_cols = ['Filename', 'Site Code', 'Country', 'Dataset', 'Version', \n",
    "                    'Duration (years)', 'Year Range', 'Size (MB)']\n",
    "    df = df[detailed_cols].sort_values(['Country', 'Site Code'])\n",
    "    \n",
    "    # Create site summary dataframe\n",
    "    site_data = []\n",
    "    for site, stats in site_stats.items():\n",
    "        years = sorted(stats['years'])\n",
    "        site_data.append({\n",
    "            'Site Code': site,\n",
    "            'Country': site.split('-')[0],\n",
    "            'Dataset Version': ', '.join(sorted(stats['versions'])),\n",
    "            'First Year': min(years),\n",
    "            'Last Year': max(years),\n",
    "            'Total Years': len(years),\n",
    "            'Coverage': f\"{min(years)}-{max(years)}\",\n",
    "            'Total Size (GB)': round(stats['size'] / (1024*1024*1024), 2)\n",
    "        })\n",
    "    \n",
    "    site_summary_df = pd.DataFrame(site_data).sort_values(['Country', 'Site Code'])\n",
    "    \n",
    "    # Create country summary dataframe\n",
    "    country_data = []\n",
    "    for country, stats in country_stats.items():\n",
    "        years = sorted(stats['years'])\n",
    "        country_data.append({\n",
    "            'Country': country,\n",
    "            'Number of Sites': len(stats['sites']),\n",
    "            'Sites': ', '.join(sorted(stats['sites'])),\n",
    "            'First Year': min(years),\n",
    "            'Last Year': max(years),\n",
    "            'Total Years': len(years),\n",
    "            'Coverage': f\"{min(years)}-{max(years)}\",\n",
    "            'Total Size (GB)': round(stats['size'] / (1024*1024*1024), 2)\n",
    "        })\n",
    "    \n",
    "    country_summary_df = pd.DataFrame(country_data).sort_values('Country')\n",
    "    \n",
    "    # Save outputs\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    detailed_output = os.path.join(output_dir, 'fluxnet_detailed_stats.csv')\n",
    "    site_output = os.path.join(output_dir, 'fluxnet_site_summary.csv')\n",
    "    country_output = os.path.join(output_dir, 'fluxnet_country_summary.csv')\n",
    "    \n",
    "    df.to_csv(detailed_output, index=False)\n",
    "    site_summary_df.to_csv(site_output, index=False)\n",
    "    country_summary_df.to_csv(country_output, index=False)\n",
    "    \n",
    "    print(\"\\n=== Analysis Complete ===\")\n",
    "    print(f\"Detailed statistics saved to: {detailed_output}\")\n",
    "    print(f\"Site summary saved to: {site_output}\")\n",
    "    print(f\"Country summary saved to: {country_output}\")\n",
    "    \n",
    "    print(\"\\nSample of Detailed Statistics:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    print(\"\\nSite Summary Statistics:\")\n",
    "    print(site_summary_df.head())\n",
    "    \n",
    "    print(\"\\nCountry Summary Statistics:\")\n",
    "    print(country_summary_df)\n",
    "\n",
    "# Run the analysis\n",
    "directory = r'C:\\Deepak\\Drought-2018 ecosystem eddy covariance flux product for 52 stations in FLUXNET-Archive format—release 2019-2'\n",
    "output_dir = os.path.join(directory, 'analysis_results')\n",
    "\n",
    "print(f\"Analyzing FLUXNET files in: {directory}\")\n",
    "results, site_stats, country_stats = analyze_fluxnet_files(directory)\n",
    "generate_outputs(results, site_stats, country_stats, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "abd71353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing FLUXNET files in: C:\\Deepak\\Warm Winter 2020 ecosystem eddy covariance flux product for 73 stations in FLUXNET-Archive format—release 2022-1\n",
      "\n",
      "Successfully processed 73 files\n",
      "\n",
      "=== Analysis Complete ===\n",
      "Detailed statistics saved to: C:\\Deepak\\Warm Winter 2020 ecosystem eddy covariance flux product for 73 stations in FLUXNET-Archive format—release 2022-1\\analysis_results\\fluxnet_detailed_stats.csv\n",
      "Site summary saved to: C:\\Deepak\\Warm Winter 2020 ecosystem eddy covariance flux product for 73 stations in FLUXNET-Archive format—release 2022-1\\analysis_results\\fluxnet_site_summary.csv\n",
      "Country summary saved to: C:\\Deepak\\Warm Winter 2020 ecosystem eddy covariance flux product for 73 stations in FLUXNET-Archive format—release 2022-1\\analysis_results\\fluxnet_country_summary.csv\n",
      "\n",
      "Sample of Detailed Statistics:\n",
      "                                            Filename Site Code Country  \\\n",
      "0  FLX_BE-Bra_FLUXNET2015_FULLSET_1996-2020_beta-...    BE-Bra      BE   \n",
      "1  FLX_BE-Dor_FLUXNET2015_FULLSET_2011-2020_beta-...    BE-Dor      BE   \n",
      "2  FLX_BE-Lcr_FLUXNET2015_FULLSET_2019-2020_beta-...    BE-Lcr      BE   \n",
      "3  FLX_BE-Lon_FLUXNET2015_FULLSET_2004-2020_beta-...    BE-Lon      BE   \n",
      "4  FLX_BE-Maa_FLUXNET2015_FULLSET_2016-2020_beta-...    BE-Maa      BE   \n",
      "\n",
      "               Dataset Version  Duration (years) Year Range  Size (MB)  \n",
      "0  FLUXNET2015_FULLSET       3                25  1996-2020     194.32  \n",
      "1  FLUXNET2015_FULLSET       3                10  2011-2020      93.58  \n",
      "2  FLUXNET2015_FULLSET       3                 2  2019-2020      23.46  \n",
      "3  FLUXNET2015_FULLSET       3                17  2004-2020     165.77  \n",
      "4  FLUXNET2015_FULLSET       3                 5  2016-2020      53.99  \n",
      "\n",
      "Site Summary Statistics:\n",
      "  Site Code Country Dataset Version  First Year  Last Year  Total Years  \\\n",
      "0    BE-Bra      BE               3        1996       2020           25   \n",
      "1    BE-Dor      BE               3        2011       2020           10   \n",
      "2    BE-Lcr      BE               3        2019       2020            2   \n",
      "3    BE-Lon      BE               3        2004       2020           17   \n",
      "4    BE-Maa      BE               3        2016       2020            5   \n",
      "\n",
      "    Coverage  Total Size (GB)  \n",
      "0  1996-2020             0.19  \n",
      "1  2011-2020             0.09  \n",
      "2  2019-2020             0.02  \n",
      "3  2004-2020             0.16  \n",
      "4  2016-2020             0.05  \n",
      "\n",
      "Country Summary Statistics:\n",
      "   Country  Number of Sites  \\\n",
      "0       BE                6   \n",
      "1       CH                6   \n",
      "2       CZ                6   \n",
      "3       DE               12   \n",
      "4       DK                2   \n",
      "5       ES                6   \n",
      "6       FI                6   \n",
      "7       FR                9   \n",
      "8       GF                1   \n",
      "9       GL                1   \n",
      "10      IE                1   \n",
      "11      IL                1   \n",
      "12      IT                9   \n",
      "13      RU                2   \n",
      "14      SE                5   \n",
      "\n",
      "                                                Sites  First Year  Last Year  \\\n",
      "0      BE-Bra, BE-Dor, BE-Lcr, BE-Lon, BE-Maa, BE-Vie        1996       2020   \n",
      "1      CH-Aws, CH-Cha, CH-Dav, CH-Fru, CH-Lae, CH-Oe2        1997       2020   \n",
      "2      CZ-BK1, CZ-KrP, CZ-Lnz, CZ-RAJ, CZ-Stn, CZ-wet        2004       2020   \n",
      "3   DE-Akm, DE-Geb, DE-Gri, DE-Hai, DE-HoH, DE-Hzd...        1996       2020   \n",
      "4                                      DK-Gds, DK-Sor        1996       2020   \n",
      "5      ES-Abr, ES-Agu, ES-Cnd, ES-LJu, ES-LM1, ES-LM2        2004       2020   \n",
      "6      FI-Hyy, FI-Ken, FI-Let, FI-Qvd, FI-Sii, FI-Var        1996       2020   \n",
      "7   FR-Aur, FR-Bil, FR-FBn, FR-Fon, FR-Gri, FR-Hes...        2004       2020   \n",
      "8                                              GF-Guy        2004       2020   \n",
      "9                                              GL-Dsk        2020       2020   \n",
      "10                                             IE-Cra        2020       2020   \n",
      "11                                             IL-Yat        2000       2020   \n",
      "12  IT-BCi, IT-BFt, IT-Cp2, IT-Lav, IT-Lsn, IT-MBo...        1999       2020   \n",
      "13                                     RU-Fy2, RU-Fyo        1998       2020   \n",
      "14             SE-Deg, SE-Htm, SE-Nor, SE-Ros, SE-Svb        2001       2020   \n",
      "\n",
      "    Total Years   Coverage  Total Size (GB)  \n",
      "0            25  1996-2020             0.74  \n",
      "1            24  1997-2020             0.97  \n",
      "2            17  2004-2020             0.63  \n",
      "3            25  1996-2020             1.51  \n",
      "4            25  1996-2020             0.23  \n",
      "5            17  2004-2020             0.55  \n",
      "6            25  1996-2020             0.51  \n",
      "7            17  2004-2020             0.95  \n",
      "8            17  2004-2020             0.09  \n",
      "9             1  2020-2020             0.02  \n",
      "10            1  2020-2020             0.02  \n",
      "11           21  2000-2020             0.17  \n",
      "12           22  1999-2020             1.05  \n",
      "13           23  1998-2020             0.28  \n",
      "14           20  2001-2020             0.46  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "def analyze_fluxnet_files(directory):\n",
    "    results = []\n",
    "    site_stats = defaultdict(lambda: {'years': set(), 'size': 0, 'versions': set()})\n",
    "    country_stats = defaultdict(lambda: {'sites': set(), 'years': set(), 'size': 0})\n",
    "    \n",
    "    for filename in os.listdir(directory):\n",
    "        if not (filename.startswith('FLX_') and filename.endswith('.zip')):\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # Split filename into components\n",
    "            parts = filename.split('_')\n",
    "            if len(parts) < 5:\n",
    "                print(f\"Skipping {filename}: not enough components\")\n",
    "                continue\n",
    "                \n",
    "            # Extract site information\n",
    "            site_code = parts[1]\n",
    "            country = site_code.split('-')[0]\n",
    "            \n",
    "            # Extract dataset version\n",
    "            version_part = parts[-1].replace('.zip', '')\n",
    "            version = version_part.split('-')[-1]  # e.g., beta-3\n",
    "            \n",
    "            # Extract year range - it's in the 4th part (index 3) after splitting\n",
    "            year_part = parts[4]  # Changed from parts[3] to parts[4]\n",
    "            start_year, end_year = map(int, year_part.split('-'))\n",
    "            duration = end_year - start_year + 1\n",
    "            \n",
    "            # Get file size\n",
    "            file_size = os.path.getsize(os.path.join(directory, filename))\n",
    "            \n",
    "            # Update statistics\n",
    "            site_stats[site_code]['years'].update(range(start_year, end_year + 1))\n",
    "            site_stats[site_code]['size'] += file_size\n",
    "            site_stats[site_code]['versions'].add(version)\n",
    "            \n",
    "            country_stats[country]['sites'].add(site_code)\n",
    "            country_stats[country]['years'].update(range(start_year, end_year + 1))\n",
    "            country_stats[country]['size'] += file_size\n",
    "            \n",
    "            results.append({\n",
    "                'Filename': filename,\n",
    "                'Site Code': site_code,\n",
    "                'Country': country,\n",
    "                'Dataset': 'FLUXNET2015_FULLSET',\n",
    "                'Version': version,\n",
    "                'Start Year': start_year,\n",
    "                'End Year': end_year,\n",
    "                'Duration (years)': duration,\n",
    "                'Year Range': f\"{start_year}-{end_year}\",\n",
    "                'Size (bytes)': file_size,\n",
    "                'Size (MB)': round(file_size / (1024*1024), 2)\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nSuccessfully processed {len(results)} files\")\n",
    "    return results, site_stats, country_stats\n",
    "\n",
    "def generate_outputs(results, site_stats, country_stats, output_dir):\n",
    "    if not results:\n",
    "        print(\"\\nNo valid FLUXNET files were processed.\")\n",
    "        return\n",
    "    \n",
    "    # Create detailed dataframe\n",
    "    df = pd.DataFrame(results)\n",
    "    detailed_cols = ['Filename', 'Site Code', 'Country', 'Dataset', 'Version', \n",
    "                    'Duration (years)', 'Year Range', 'Size (MB)']\n",
    "    df = df[detailed_cols].sort_values(['Country', 'Site Code'])\n",
    "    \n",
    "    # Create site summary dataframe\n",
    "    site_data = []\n",
    "    for site, stats in site_stats.items():\n",
    "        years = sorted(stats['years'])\n",
    "        site_data.append({\n",
    "            'Site Code': site,\n",
    "            'Country': site.split('-')[0],\n",
    "            'Dataset Version': ', '.join(sorted(stats['versions'])),\n",
    "            'First Year': min(years),\n",
    "            'Last Year': max(years),\n",
    "            'Total Years': len(years),\n",
    "            'Coverage': f\"{min(years)}-{max(years)}\",\n",
    "            'Total Size (GB)': round(stats['size'] / (1024*1024*1024), 2)\n",
    "        })\n",
    "    \n",
    "    site_summary_df = pd.DataFrame(site_data).sort_values(['Country', 'Site Code'])\n",
    "    \n",
    "    # Create country summary dataframe\n",
    "    country_data = []\n",
    "    for country, stats in country_stats.items():\n",
    "        years = sorted(stats['years'])\n",
    "        country_data.append({\n",
    "            'Country': country,\n",
    "            'Number of Sites': len(stats['sites']),\n",
    "            'Sites': ', '.join(sorted(stats['sites'])),\n",
    "            'First Year': min(years),\n",
    "            'Last Year': max(years),\n",
    "            'Total Years': len(years),\n",
    "            'Coverage': f\"{min(years)}-{max(years)}\",\n",
    "            'Total Size (GB)': round(stats['size'] / (1024*1024*1024), 2)\n",
    "        })\n",
    "    \n",
    "    country_summary_df = pd.DataFrame(country_data).sort_values('Country')\n",
    "    \n",
    "    # Save outputs\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    detailed_output = os.path.join(output_dir, 'fluxnet_detailed_stats.csv')\n",
    "    site_output = os.path.join(output_dir, 'fluxnet_site_summary.csv')\n",
    "    country_output = os.path.join(output_dir, 'fluxnet_country_summary.csv')\n",
    "    \n",
    "    df.to_csv(detailed_output, index=False)\n",
    "    site_summary_df.to_csv(site_output, index=False)\n",
    "    country_summary_df.to_csv(country_output, index=False)\n",
    "    \n",
    "    print(\"\\n=== Analysis Complete ===\")\n",
    "    print(f\"Detailed statistics saved to: {detailed_output}\")\n",
    "    print(f\"Site summary saved to: {site_output}\")\n",
    "    print(f\"Country summary saved to: {country_output}\")\n",
    "    \n",
    "    print(\"\\nSample of Detailed Statistics:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    print(\"\\nSite Summary Statistics:\")\n",
    "    print(site_summary_df.head())\n",
    "    \n",
    "    print(\"\\nCountry Summary Statistics:\")\n",
    "    print(country_summary_df)\n",
    "\n",
    "# Run the analysis\n",
    "directory = r'C:\\Deepak\\Warm Winter 2020 ecosystem eddy covariance flux product for 73 stations in FLUXNET-Archive format—release 2022-1'\n",
    "output_dir = os.path.join(directory, 'analysis_results')\n",
    "\n",
    "print(f\"Analyzing FLUXNET files in: {directory}\")\n",
    "results, site_stats, country_stats = analyze_fluxnet_files(directory)\n",
    "generate_outputs(results, site_stats, country_stats, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8ecba86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined file saved as 'combined_sites_years.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "def load_and_preprocess(file_path, name):\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Standardize column names\n",
    "    if 'Site' in df.columns:\n",
    "        df = df.rename(columns={'Site': 'Site Code'})\n",
    "    \n",
    "    # Convert size columns to GB for consistency\n",
    "    if 'Total Size (MB)' in df.columns:\n",
    "        df['Total Size (GB)'] = df['Total Size (MB)'] / 1024\n",
    "        df = df.drop(columns=['Total Size (MB)'])\n",
    "    \n",
    "    # Create year range column for each source\n",
    "    df[name + '_Years'] = df['First Year'].astype(str) + '-' + df['Last Year'].astype(str)\n",
    "    \n",
    "    return df[['Site Code', 'Country', name + '_Years']]\n",
    "\n",
    "# Load all files\n",
    "warm_df = load_and_preprocess('Warm_icos_.csv', 'Warm')\n",
    "drought_df = load_and_preprocess('drought_icos.csv', 'Drought')\n",
    "fluxcom_df = load_and_preprocess('fluxcom_sites.csv', 'Fluxcom')\n",
    "icos2025_df = load_and_preprocess('2025_icos.csv', 'ICOS2025')\n",
    "\n",
    "# Merge all data\n",
    "combined = pd.merge(\n",
    "    pd.merge(\n",
    "        pd.merge(\n",
    "            icos2025_df, \n",
    "            drought_df, \n",
    "            on=['Site Code', 'Country'], \n",
    "            how='outer'\n",
    "        ),\n",
    "        fluxcom_df,\n",
    "        on=['Site Code', 'Country'], \n",
    "        how='outer'\n",
    "    ),\n",
    "    warm_df,\n",
    "    on=['Site Code', 'Country'], \n",
    "    how='outer'\n",
    ")\n",
    "\n",
    "# Function to parse year ranges and combine them\n",
    "def combine_years(row):\n",
    "    sources = ['ICOS2025', 'Drought', 'Fluxcom', 'Warm']\n",
    "    all_years = set()\n",
    "    \n",
    "    for source in sources:\n",
    "        year_range = row.get(source + '_Years')\n",
    "        if pd.notna(year_range):\n",
    "            try:\n",
    "                start, end = map(int, year_range.split('-'))\n",
    "                all_years.update(range(start, end + 1))\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    if not all_years:\n",
    "        return None, None, None, None\n",
    "    \n",
    "    first_year = min(all_years)\n",
    "    last_year = max(all_years)\n",
    "    total_years = len(all_years)\n",
    "    coverage = f\"{first_year}-{last_year}\"\n",
    "    \n",
    "    return first_year, last_year, total_years, coverage\n",
    "\n",
    "# Apply the combining function\n",
    "combined[['Combined_First_Year', 'Combined_Last_Year', \n",
    "          'Combined_Total_Years', 'Combined_Coverage']] = combined.apply(\n",
    "    lambda row: pd.Series(combine_years(row)), axis=1)\n",
    "\n",
    "# Reorder columns and sort\n",
    "output_columns = [\n",
    "    'Site Code', 'Country', \n",
    "    'ICOS2025_Years', 'Drought_Years', 'Fluxcom_Years', 'Warm_Years',\n",
    "    'Combined_First_Year', 'Combined_Last_Year', 'Combined_Total_Years', 'Combined_Coverage'\n",
    "]\n",
    "\n",
    "result = combined[output_columns].sort_values(by=['Country', 'Site Code'])\n",
    "\n",
    "# Save to CSV\n",
    "result.to_csv('combined_sites_years.csv', index=False)\n",
    "print(\"Combined file saved as 'combined_sites_years.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "292de2eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted file saved as 'combined_sites_years_sorted.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the final combined CSV (if already generated)\n",
    "df = pd.read_csv('combined_sites_years.csv')\n",
    "\n",
    "# OPTION 1: Sort by Combined First Year (oldest to newest)\n",
    "df_sorted = df.sort_values(by='Combined_Total_Years' ,ascending=False)\n",
    "\n",
    "# OPTION 2: Sort by Combined Total Years (longest to shortest coverage)\n",
    "# df_sorted = df.sort_values(by='Combined_Total_Years', ascending=False)\n",
    "\n",
    "# Save the sorted version\n",
    "df_sorted.to_csv('combined_sites_years_sorted.csv', index=False)\n",
    "print(\"Sorted file saved as 'combined_sites_years_sorted.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53f5d9ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined file (without drought) saved as 'combined_sites_years_no_drought.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "def load_and_preprocess(file_path, name):\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Standardize column names\n",
    "    if 'Site' in df.columns:\n",
    "        df = df.rename(columns={'Site': 'Site Code'})\n",
    "    \n",
    "    # Convert size columns to GB for consistency\n",
    "    if 'Total Size (MB)' in df.columns:\n",
    "        df['Total Size (GB)'] = df['Total Size (MB)'] / 1024\n",
    "        df = df.drop(columns=['Total Size (MB)'])\n",
    "    \n",
    "    # Create year range column for each source\n",
    "    df[name + '_Years'] = df['First Year'].astype(str) + '-' + df['Last Year'].astype(str)\n",
    "    \n",
    "    return df[['Site Code', 'Country', name + '_Years']]\n",
    "\n",
    "# Load all files EXCEPT drought\n",
    "warm_df = load_and_preprocess('Warm_icos_.csv', 'Warm')\n",
    "fluxcom_df = load_and_preprocess('fluxcom_sites.csv', 'Fluxcom')\n",
    "icos2025_df = load_and_preprocess('2025_icos.csv', 'ICOS2025')\n",
    "\n",
    "# Merge data WITHOUT drought\n",
    "combined = pd.merge(\n",
    "    pd.merge(\n",
    "        icos2025_df,\n",
    "        fluxcom_df,\n",
    "        on=['Site Code', 'Country'], \n",
    "        how='outer'\n",
    "    ),\n",
    "    warm_df,\n",
    "    on=['Site Code', 'Country'], \n",
    "    how='outer'\n",
    ")\n",
    "\n",
    "# Function to parse year ranges and combine them (excluding drought)\n",
    "def combine_years(row):\n",
    "    sources = ['ICOS2025', 'Fluxcom', 'Warm']  # Removed 'Drought'\n",
    "    all_years = set()\n",
    "    \n",
    "    for source in sources:\n",
    "        year_range = row.get(source + '_Years')\n",
    "        if pd.notna(year_range):\n",
    "            try:\n",
    "                start, end = map(int, year_range.split('-'))\n",
    "                all_years.update(range(start, end + 1))\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    if not all_years:\n",
    "        return None, None, None, None\n",
    "    \n",
    "    first_year = min(all_years)\n",
    "    last_year = max(all_years)\n",
    "    total_years = len(all_years)\n",
    "    coverage = f\"{first_year}-{last_year}\"\n",
    "    \n",
    "    return first_year, last_year, total_years, coverage\n",
    "\n",
    "# Apply the combining function\n",
    "combined[['Combined_First_Year', 'Combined_Last_Year', \n",
    "          'Combined_Total_Years', 'Combined_Coverage']] = combined.apply(\n",
    "    lambda row: pd.Series(combine_years(row)), axis=1)\n",
    "\n",
    "# Reorder columns (removed Drought_Years)\n",
    "output_columns = [\n",
    "    'Site Code', 'Country', \n",
    "    'ICOS2025_Years', 'Fluxcom_Years', 'Warm_Years',\n",
    "    'Combined_First_Year', 'Combined_Last_Year', 'Combined_Total_Years', 'Combined_Coverage'\n",
    "]\n",
    "\n",
    "result = combined[output_columns].sort_values(by='Combined_Total_Years' ,ascending=False)\n",
    "\n",
    "# Save to CSV\n",
    "result.to_csv('dataset_availability_final.csv', index=False)\n",
    "print(\"Combined file (without drought) saved as 'combined_sites_years_no_drought.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6fc11bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined file (Warm + ICOS2025 only) saved as 'combined_sites_years_warm_icos_only.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "def load_and_preprocess(file_path, name):\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Standardize column names\n",
    "    if 'Site' in df.columns:\n",
    "        df = df.rename(columns={'Site': 'Site Code'})\n",
    "    \n",
    "    # Convert size columns to GB for consistency\n",
    "    if 'Total Size (MB)' in df.columns:\n",
    "        df['Total Size (GB)'] = df['Total Size (MB)'] / 1024\n",
    "        df = df.drop(columns=['Total Size (MB)'])\n",
    "    \n",
    "    # Create year range column for each source\n",
    "    df[name + '_Years'] = df['First Year'].astype(str) + '-' + df['Last Year'].astype(str)\n",
    "    \n",
    "    return df[['Site Code', 'Country', name + '_Years']]\n",
    "\n",
    "# Load only Warm and ICOS2025 data (no drought or fluxcom)\n",
    "warm_df = load_and_preprocess('Warm_icos_.csv', 'Warm')\n",
    "icos2025_df = load_and_preprocess('2025_icos.csv', 'ICOS2025')\n",
    "\n",
    "# Merge only the two datasets\n",
    "combined = pd.merge(\n",
    "    icos2025_df,\n",
    "    warm_df,\n",
    "    on=['Site Code', 'Country'], \n",
    "    how='outer'\n",
    ")\n",
    "\n",
    "# Function to parse year ranges and combine them (only ICOS2025 and Warm)\n",
    "def combine_years(row):\n",
    "    sources = ['ICOS2025', 'Warm']  # Only these two sources\n",
    "    all_years = set()\n",
    "    \n",
    "    for source in sources:\n",
    "        year_range = row.get(source + '_Years')\n",
    "        if pd.notna(year_range):\n",
    "            try:\n",
    "                start, end = map(int, year_range.split('-'))\n",
    "                all_years.update(range(start, end + 1))\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    if not all_years:\n",
    "        return None, None, None, None\n",
    "    \n",
    "    first_year = min(all_years)\n",
    "    last_year = max(all_years)\n",
    "    total_years = len(all_years)\n",
    "    coverage = f\"{first_year}-{last_year}\"\n",
    "    \n",
    "    return first_year, last_year, total_years, coverage\n",
    "\n",
    "# Apply the combining function\n",
    "combined[['Combined_First_Year', 'Combined_Last_Year', \n",
    "          'Combined_Total_Years', 'Combined_Coverage']] = combined.apply(\n",
    "    lambda row: pd.Series(combine_years(row)), axis=1)\n",
    "\n",
    "# Reorder columns (only ICOS2025 and Warm)\n",
    "output_columns = [\n",
    "    'Site Code', 'Country', \n",
    "    'ICOS2025_Years', 'Warm_Years',\n",
    "    'Combined_First_Year', 'Combined_Last_Year', 'Combined_Total_Years', 'Combined_Coverage'\n",
    "]\n",
    "\n",
    "result = combined[output_columns].sort_values(by='Combined_Total_Years' ,ascending=False)\n",
    "\n",
    "# Save to CSV\n",
    "result.to_csv('combined_sites_years_warm_icos_only.csv', index=False)\n",
    "print(\"Combined file (Warm + ICOS2025 only) saved as 'combined_sites_years_warm_icos_only.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dcf6049d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Sites (11):\n",
      "Site Code Country Combined_Coverage\n",
      "   GF-Guy      GF         2004-2025\n",
      "   IT-OXm      IT         2004-2025\n",
      "   DE-Gri      DE         2004-2025\n",
      "   DE-Kli      DE         2004-2025\n",
      "   FR-Aur      FR         2005-2025\n",
      "   CZ-wet      CZ         2006-2025\n",
      "   IT-Tor      IT         2008-2025\n",
      "   FI-Let      FI         2009-2025\n",
      "   DE-Hzd      DE         2010-2025\n",
      "   DE-RuR      DE         2010-2025\n",
      "   IT-TrF      IT         2011-2025\n",
      "   DE-RuW      DE         2011-2025\n",
      "   IT-Lsn      IT         2015-2025\n",
      "   CH-BaK      CH         2015-2025\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "site_stats = pd.read_csv('site_statistics.csv')\n",
    "data_avail = pd.read_csv('dataset_availability_2025.csv')\n",
    "\n",
    "# Find sites in data_avail that aren't in site_stats\n",
    "missing_sites = data_avail[~data_avail['Site Code'].isin(site_stats['Site Code'])]\n",
    "\n",
    "# Display all missing sites (11 total)\n",
    "print(\"Missing Sites (11):\")\n",
    "print(missing_sites[['Site Code', 'Country', 'Combined_Coverage']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a441d7",
   "metadata": {},
   "source": [
    "FLX_BE-Bra_FLUXNET2015_FULLSET_DD_1996-2020_beta-3.csv\n",
    "FLX_BE-Bra_FLUXNET2015_FULLSET_HH_1996-2020_beta-3.csv\n",
    "FLX_BE-Bra_FLUXNET2015_FULLSET_MM_1996-2020_beta-3.csv\n",
    "FLX_BE-Bra_FLUXNET2015_FULLSET_WW_1996-2020_beta-3.csv\n",
    "FLX_BE-Bra_FLUXNET2015_FULLSET_YY_1996-2020_beta-3.csv\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
